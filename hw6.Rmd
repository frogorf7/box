---
title: "HW6"
author: "Jinwoo Jeong"
date: '2022 4 18 '
output:
  pdf_document: default
  html_document: default
---

```{r, echo=FALSE , warning=FALSE}
library(ISLR2)
library(leaps)
library(caret)
```


# Question 3
a)
```{r}
set.seed(4052)
x = rnorm(100)
eps = rnorm(100)
```

b)

```{r}
b0 = 1
b1 = 2
b2 = 3
b3 = 4
y <- b0 + b1 * x + b2 * x^2 + b3 * x^3 + eps
```


c)
```{r}
data=data.frame(y,x)
fit=regsubsets(y ~ poly(x,10), data = data, nvmax = 10)
fit_summary=summary(fit)
par(mfrow = c(1,3))
plot(fit_summary$cp, xlab = "Number of variables", ylab = "Cp", type = "b")
plot(fit_summary$bic, xlab = "Number of variables", ylab = "BIC", type = "b")
plot(fit_summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R2", type = "b")

```
Among models we pick model with the 3 variables using $C_p$ , BIC and adjusted $R^2$


d)
```{r}
fit=regsubsets(y ~ poly(x,10), data = data, nvmax = 10,method = "forward")
fit_summary=summary(fit)
par(mfrow = c(1,3))
plot(fit_summary$cp, xlab = "Number of variables", ylab = "Cp", type = "b")
plot(fit_summary$bic, xlab = "Number of variables", ylab = "BIC", type = "b")
plot(fit_summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R2", type = "b")
```


```{r}
fit=regsubsets(y ~ poly(x,10), data = data, nvmax = 10,method = "backward")
fit_summary = summary(fit)
par(mfrow = c(1,3))
plot(fit_summary$cp, xlab = "Number of variables", ylab = "Cp", type = "b")
plot(fit_summary$bic, xlab = "Number of variables", ylab = "BIC", type = "b")
plot(fit_summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R2", type = "b")
```
Among models we pick model with the 3 variables using $C_p$ , BIC and adjusted $R^2$ both using forward and backward selection. 


e)
```{r}
library(glmnet)

xmat = model.matrix(y ~ poly(x,10), data = data)[,-1]

grid=10^seq(10, -2, length = 100)
lasso=glmnet(xmat, y,  alpha = 1,lambda = grid)
cv.lasso=cv.glmnet(xmat, y,  alpha = 1, lambda = grid,type.measure = "mse")
plot(cv.lasso)
best_lambda  = cv.lasso$lambda.min
lasso.coef=predict.glmnet(lasso, type = "coefficients",s = best_lambda)[1:11,]
lasso.coef
```

Lasso picks $X,X^2,X^3$ as the variable of the model

f)

```{r}
b7 = 8
y7 = b0 + b7 * x^7 + eps
data7=data.frame(y= y7,x= x)
fit2=regsubsets(y7 ~ poly(x,10), data = data7, nvmax = 10)
fit_summary2 = summary(fit2)
par(mfrow = c(1,3))
plot(fit_summary2$cp, xlab = "Number of variables", ylab = "Cp", type = "b")
plot(fit_summary2$bic, xlab = "Number of variables", ylab = "BIC", type = "b")
plot(fit_summary2$adjr2, xlab = "Number of variables", ylab = "Adjusted R2", type = "b")

```
```{r}

xmat=model.matrix(y7 ~ poly(x,10), data = data7)[, -1]
cv.lasso=cv.glmnet(xmat, y7, alpha = 1,type.measure = "mse")
best_lambda=cv.lasso$lambda.min
fit.lasso=glmnet(xmat, y7, alpha = 1)
lasso.coef=predict.glmnet(fit.lasso, type = "coefficients",s = best_lambda)[1:11,]
lasso.coef
```
subset select the model with 7 variable lasso select model with $X,X^2,X^3,X^4,X^5$ as variables


# Question 4

a)
```{r}
library(ISLR)

data(College)
set.seed(4052)
n = nrow(College)
train = sample(1:n, n/2)
test = -train
ctrain = College[train, ]
ctest = College[test, ]
```

b)
```{r}
fit.lm=lm(Apps ~ ., data = ctrain)
pred.lm=predict(fit.lm, ctest)
mean((pred.lm - ctest$Apps)^2)
```
The test MSE is 1384845


c)
```{r}
train.mat=model.matrix(Apps ~ ., data = ctrain)
test.mat=model.matrix(Apps ~ ., data = ctest)
grid=10^seq(10, -2, length = 100)
fit.ridge=glmnet(train.mat, ctrain$Apps, alpha = 0, lambda = grid)
cv.ridge=cv.glmnet(train.mat, ctrain$Apps, alpha = 0, lambda = grid)
best_lambda=cv.ridge$lambda.min
pred.ridge=predict(fit.ridge, s = best_lambda, newx = test.mat)
mean((pred.ridge - ctest$Apps)^2)
```

The test MSE is 1482466 which is greater than least squares


d)
```{r}
fit.lasso=glmnet(train.mat, ctrain$Apps, alpha = 1, lambda = grid)
cv.lasso=cv.glmnet(train.mat, ctrain$Apps, alpha = 1, lambda = grid)
best_lambda=cv.lasso$lambda.min
pred.lasso=predict(fit.lasso, s = best_lambda, newx = test.mat)
mean((pred.lasso - ctest$Apps)^2)
predict(fit.lasso, s = best_lambda, type = "coefficients")
```

The test MSE is 23615357 which is greater than least squares and ridge regression\
there are 11 variables selected by the lasso model


# Question 5
```{r, echo=FALSE, warning=FALSE}
library(MASS)
data(Boston)
train= sample(1:nrow(Boston), nrow(Boston)/2)
test= -train
y.test=Boston$crim[test]
```

a)
best subset
```{r}
set.seed(4052)

bestsub=regsubsets(crim ~ ., data=Boston, subset=train, nvmax=13)
bestsub.sum=summary(bestsub)
xmat=model.matrix(crim~., data=Boston[test,])

val.errors <- rep(NA,13)
for(i in 1:13)
{
 coefi=coef(bestsub,id=i)
 pred=xmat[,names(coefi)]%*%coefi
 val.errors[i]=mean((Boston$crim[test]-pred)^2)
}

which.min(val.errors)
plot(val.errors, type='b')
bs.mse = val.errors[4]
bs.mse
```


```{r}
forw=regsubsets(crim~., data=Boston, subset=train, nvmax=13, method="forward")
xmat=model.matrix(crim~., data=Boston[test,])

val.errors=rep(NA,13)
for(i in 1:13)
{
 coefi=coef(forw,id=i)
 pred=xmat[,names(coefi)]%*%coefi
 val.errors[i]=mean((Boston$crim[test]-pred)^2) 
}

which.min(val.errors)
plot(val.errors, type='b')
forw.mse <- val.errors[2]
forw.mse
```
```{r}
back=regsubsets(crim~., data=Boston, subset=train, nvmax=13, method="backward")
xmat=model.matrix(crim~., data=Boston[test,])

val.errors <- rep(NA,13)
for(i in 1:13){
 coefi=coef(back,id=i)
 pred=xmat[,names(coefi)]%*%coefi
 val.errors[i]=mean((Boston$crim[test]-pred)^2) 
}

which.min(val.errors)
plot(val.errors, type='b')
back.mse <- val.errors[4]
back.mse
```


lasso
```{r}
x = model.matrix(crim ~ . , data = Boston)[,-1]
y = Boston$crim
lasso.model = cv.glmnet(x, y, alpha=1,type.measure = "mse")
best_lambda <- lasso.model$lambda.min
plot(lasso.model)
coef(lasso.model)
lasso.pred=predict(lasso.model, s=best_lambda, newx=x[test,]) 
lasso.fit = glmnet(x, y, alpha=1)
lasso.coef=predict.glmnet(lasso.fit, type = "coefficients",s = best_lambda)[1:14,]
lasso.coef
lasso.error=mean((lasso.pred-y[test])^2)
lasso.error
length(lasso.coef[lasso.coef != 0])-1
```


ridge
```{r}
x = model.matrix(crim ~ ., data = Boston)[,-1]
y = Boston$crim
ridge.model = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(ridge.model)
best_lambda <- ridge.model$lambda.min
ridge.pred <- predict(ridge.model, s=best_lambda, newx=x[test,]) 
ridge.fit = glmnet(x, y, alpha=0)
ridge.coef=predict.glmnet(ridge.fit, type = "coefficients",s = best_lambda)[1:14,]
ridge.error <- mean((ridge.pred-y[test])^2)
ridge.error
ridge.coef
length(ridge.coef[ridge.coef != 0])-1


```

b)

```{r}
errors = c(bs.mse, forw.mse, back.mse , lasso.error, ridge.error)
names(errors)= c("best subset" , "fw selection","bk selection" , "lasso" , "rigde")
barplot(sort(errors, decreasing = T), las = 1)

```
Among 5 models the lasso has the smallest MSE with 11 variables

c)

No the model that I chose contains 11 variables zn indus chas nox rm dis rad ptratio black lstat medv. According to the lasso model using 11 variables minimize the error. 
